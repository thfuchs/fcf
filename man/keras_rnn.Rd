% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/keras_rnn.R
\name{keras_rnn}
\alias{keras_rnn}
\title{Train Recurrent Neural Network with Gated Recurrent Unit (GRU) or
Long-Short Term Memory (LSTM) using Keras framework}
\usage{
keras_rnn(
  X,
  Y,
  model_type,
  tsteps,
  n_epochs = 200,
  n_units = 32,
  loss = "mse",
  metrics = NULL,
  dropout_in_test = FALSE,
  optimizer = optimizer_rmsprop(),
  dropout = 0,
  recurrent_dropout = 0,
  live_plot = FALSE
)
}
\arguments{
\item{X}{list of "train", "val", and "test" with 3D (keras) arrays}

\item{Y}{list of "train", "val", and "test" with 2D (keras) arrays}

\item{model_type}{One of "simple", "gru" and "lstm"}

\item{tsteps}{number of time steps for keras input shape}

\item{n_epochs}{default 200}

\item{n_units}{32 (currently fixed)}

\item{loss}{default "mse"}

\item{metrics}{default NULL}

\item{dropout_in_test}{apply dropout during training only (default) or during
testing also? Required for dropout-based prediction intervals (bayesian RNN)}

\item{optimizer}{from keras, e.g. \code{optimizer_rmsprop()}}

\item{dropout}{dropout rate}

\item{recurrent_dropout}{Dropout rate applied to reccurent layer. Default 0}

\item{live_plot}{plot loss and validation metric during training? False by
default}
}
\value{
evaluation scores for training, validation and test set
}
\description{
Train Recurrent Neural Network with Gated Recurrent Unit (GRU) or
Long-Short Term Memory (LSTM) using Keras framework
}
